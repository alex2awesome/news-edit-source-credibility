model: "llama3"
backend: "ollama"
ollama_api_base: "http://localhost:11434"
temperature: 0.0
spacy_model: en_core_web_sm
max_tokens: 8192
batch_size: 4
hedge_window_tokens: 80
accept_confidence_min: 3
out_root: "./out"
cache_raw_responses: true
skip_if_cached: true
min_versions: 2
max_versions: 20
cleanup_cached_dirs: true
