model: "llama-3.2-70b-instruct"
vllm_api_base: "http://localhost:8000/v1"
temperature: 0.0
max_tokens: 2048
batch_size: 4
hedge_window_tokens: 80
accept_confidence_min: 3
out_root: "./out"
cache_raw_responses: true
skip_if_cached: true
min_versions: 2
max_versions: 20
cleanup_cached_dirs: true
