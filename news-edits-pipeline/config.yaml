model: "meta-llama/Llama-3.3-70B-Instruct"
vllm_api_base: "http://localhost:8000/v1"
spacy_model: en_core_web_lg
temperature: 0.0
max_tokens: 8192
batch_size: 1
hedge_window_tokens: 80
accept_confidence_min: 3
cache_raw_responses: true
skip_if_cached: true
min_versions: 2
max_versions: 20
cleanup_cached_dirs: true
skip_similarity_threshold: 0.95
llm_retries: 3
llm_retry_backoff_seconds: 2.0
